{"cells":[{"cell_type":"markdown","id":"f5e56428","metadata":{"id":"f5e56428"},"source":["# **Introduction aux CNNs** /  *Introduction to CNNs*"]},{"cell_type":"markdown","id":"1027cef7","metadata":{"id":"1027cef7"},"source":["## VSCode - Configuration / *Setup*"]},{"cell_type":"markdown","id":"f56f0289","metadata":{"id":"f56f0289"},"source":["**1. Activer le solver libmamba pour éviter les problèmes liés réseau / activate libmamba solver to avoid network issue**\n","\n","conda install -n base -y -c conda-forge conda-libmamba-solver\n","\n","conda config --set solver libmamba\n","\n","conda config --add channels conda-forge\n","\n","conda config --set channel_priority strict\n","\n","**2. Créer un env dédié au TP / creat dedicated TP environment**\n","\n","conda create -n TP -y -c conda-forge python=3.12 opencv ipykernel jupyterlab\n","\n","conda install pytorch torchvision\n","\n","conda install matplotlib\n","\n","conda install grad-cam\n","\n","**3. Activer l’env (et lancer Jupyter Lab) / Activate env (and run Jupyter Lab, if from vscode)**\n","\n","conda activate TP\n","\n","option: jupyter lab"]},{"cell_type":"markdown","id":"c706e6da","metadata":{"id":"c706e6da"},"source":["## Présentation / *Overview*"]},{"cell_type":"markdown","id":"28fa0a11","metadata":{"id":"28fa0a11"},"source":["*Ce TP est tiré des tutoriels Torch (https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) et de l'utilisation de grad-cam pour la visualisation des caractéristiques (https://github.com/utkuozbulak/pytorch-cnn-visualizations?tab=readme-ov-file#convolutional-neural-network-filter-visualization)*"]},{"cell_type":"markdown","id":"c4b2d897","metadata":{"id":"c4b2d897"},"source":["### Description"]},{"cell_type":"markdown","id":"07dd5c49","metadata":{"id":"07dd5c49"},"source":["Concrètement, une convolution est un filtre spatial. Un CNN (convolutional neural network) applique des noyaux (kernels) appris. En traitement d'image traditionnel, nous avons pu aborder différents type de filtres (Laplacien gaussien, bruts, périodiques, ...) qui permettent la détection de bords, de flou, ou de netteté.\n","Ici, un CNN apprendra les bons filtres à partir des données plutôt que d'explicitement lui donner l'instruction.\n","\n","Dans les automatismes des CNNs, nous retrouvons également le pooling, qui est une réduction de résolution. Cela correspond au downsampling, et le CNN l'intègre comme un résumé de features.\n","\n","Le réseau complet correspond à des couches successives de transformations d'images (features map):\n","\n","1. Détection de bords / textures simpels\n","2. Motifs complexes (coins, textures, cercles)\n","3. Parties d'objets (yeux, roues, etc).\n","\n","-------\n","*Concretely, a convolution is a spatial filter. A convolutional neural network (CNN) applies learned kernels. In traditional image processing, we’ve covered various types of filters (Laplacian of Gaussian, basic, periodic, …) that enable edge detection, blurring, or sharpening. Here, a CNN learns the appropriate filters from data rather than being explicitly instructed.*\n","*\n","In the usual CNN machinery, we also find pooling, which reduces resolution. This corresponds to downsampling, and the CNN uses it as a summary of features.*\n","\n","*The full network amounts to successive layers of image transformations (feature maps):*\n","1. *Edge detection / simple textures*\n","2. *Complex patterns (corners, textures, circles)*\n","3. *Object parts (eyes, wheels, etc.).*"]},{"cell_type":"markdown","id":"05ce7da1","metadata":{"id":"05ce7da1"},"source":["### Plan / Summary"]},{"cell_type":"markdown","id":"a1afc620","metadata":{"id":"a1afc620"},"source":["Dans ce TP:\n","1. Charger et normaliser les ensembles de données de formation et de test CIFAR10 (TL-10) à l'aide de torchvision\n","2. Définir un réseau neuronal convolutif\n","3. Définir une fonction de perte\n","4. Entraîner le réseau sur les données d'entraînement\n","5. Tester le réseau sur les données de test\n","6. Explorer le modèle avec grad-cam\n","\n","------\n","In this lab:\n","\n","1. *Load and normalize the CIFAR10 training and test datasets using torchvision*\n","2. *Define a Convolutional Neural Network*\n","3. *Define a loss function*\n","4. *Train the network on the training data*\n","5. *Test the network on the test data*\n","6. *Explore the model with grad-cam*"]},{"cell_type":"markdown","id":"f6440ac4","metadata":{"id":"f6440ac4"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"id":"ebc3dcb7","metadata":{"id":"ebc3dcb7"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms"]},{"cell_type":"markdown","id":"8987f80c","metadata":{"id":"8987f80c"},"source":["## 1. Charger et normaliser les ensembles de données de formation et de test CIFAR10 (TL-10)\n","## 1. *Load and normalize the CIFAR10 (TCL-10) training and test datasets using torchvision*"]},{"cell_type":"markdown","id":"8941f5ba","metadata":{"id":"8941f5ba"},"source":["### **/!\\ Run the TP with CIRFAR10. Then try to play with STL-10 and improve CNN performance**"]},{"cell_type":"markdown","id":"f30b685d","metadata":{"id":"f30b685d"},"source":["Pour ce tutoriel, nous utiliserons le jeu de données CIFAR10 ou STL-10. Il contient les classes suivantes : « avion », « automobile », « oiseau », « chat », « cerf », « chien », « grenouille », « cheval », « bateau » et « camion ». Les images CIFAR-10 ont une taille de 3 x 32 x 32 (3 x 96 x 96 pour STL-10), soit des images couleur à trois canaux de 32 x 32 pixels (96 x 96 pour STL-10).\n","\n","Attention : les architectures ne sont pas les mêmes pour STL-10 et CIFAR10.\n","\n","------\n","\n","*For this tutorial, we will use the CIFAR10 or STL-10 dataset. It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32 (3 x 96 x 96 for STL-10), i.e. 3-channel color images of 32x32 (96 x 96 for STL-10)pixels in size.*\n","\n","*Warning: architectures are not the same between STL-10 and CIFAR10*"]},{"cell_type":"markdown","id":"7931248b","metadata":{"id":"7931248b"},"source":["1. CIFAR10"]},{"cell_type":"code","execution_count":1,"id":"2ecf41bb","metadata":{"id":"2ecf41bb","executionInfo":{"status":"error","timestamp":1760950330112,"user_tz":-120,"elapsed":28,"user":{"displayName":"Swann Ruyter","userId":"09099854111043013404"}},"outputId":"e0fc5103-10f6-434a-926e-3783bd925774","colab":{"base_uri":"https://localhost:8080/","height":211}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'transforms' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2767995521.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transform = transforms.Compose(\n\u001b[0m\u001b[1;32m      2\u001b[0m     [transforms.ToTensor(),\n\u001b[1;32m      3\u001b[0m      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"]}],"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","batch_size = 4\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"]},{"cell_type":"markdown","id":"533c1f29","metadata":{"id":"533c1f29"},"source":["2. STL-10 (to uncomment)"]},{"cell_type":"code","execution_count":null,"id":"4dc8a60c","metadata":{"id":"4dc8a60c"},"outputs":[],"source":["from torchvision.datasets import STL10\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5),\n","                         (0.5, 0.5, 0.5))\n","])\n","\n","batch_size = 4\n","\n","trainset = STL10(root='./data', split='train',\n","                 download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = STL10(root='./data', split='test',\n","                download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","\n","# STL-10 a 10 classes (remplace 'frog' par 'monkey')\n","classes = trainset.classes  # ['airplane','bird','car','cat','deer','dog','horse','monkey','ship','truck']\n"]},{"cell_type":"markdown","id":"90a06af6","metadata":{"id":"90a06af6"},"source":["/!\\ If running on Windows and you get a BrokenPipeError, try setting the num_worker of torch.utils.data.DataLoader() to 0."]},{"cell_type":"markdown","id":"06e795ee","metadata":{"id":"06e795ee"},"source":["-> Montrer quelques images de formation"]},{"cell_type":"code","execution_count":null,"id":"277fc456","metadata":{"id":"277fc456"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# functions to show an image\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","# get some random training images\n","dataiter = iter(trainloader)\n","images, labels = next(dataiter)\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images))\n","# print labels\n","print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"]},{"cell_type":"markdown","id":"ab2c8e94","metadata":{"id":"ab2c8e94"},"source":["## 2. Définir un réseau neuronal convolutif / 2. *Define a Convolutional Neural Network*"]},{"cell_type":"markdown","id":"b17c8b38","metadata":{"id":"b17c8b38"},"source":["1. CNN pour CIFAR10 / *CNN for CIFAR10*"]},{"cell_type":"code","execution_count":null,"id":"7973c846","metadata":{"id":"7973c846"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5) #TODO: nn.Conv2d(3, 16, 5)  # adapt next one\n","        self.pool = nn.MaxPool2d(2, 2) #TODO: self.pool = nn.Identity() / nn.AvgPool2d(2,2)\n","        self.conv2 = nn.Conv2d(6, 16, 5) #TODO: nn.Conv2d(..., kernel_size=3, padding=1) / nn.Conv2d(..., kernel_size=7, padding=3)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","\n","        x = F.adaptive_avg_pool2d(x, (5, 5))\n","\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","\n","net = Net()\n","\n","# === OPTIONAL: quick knobs for the lab (uncomment one at a time) ===\n","\n","# 1) More filters (observe more/varied feature maps)\n","# net.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=0)\n","# net.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=0); net.force_5x5 = True\n","\n","# 2) Different kernel sizes (keep size with padding)\n","# net.conv1 = nn.Conv2d(3, 6, kernel_size=3, padding=1)\n","# net.conv2 = nn.Conv2d(6, 16, kernel_size=7, padding=3); net.force_5x5 = True\n","\n","# 3) Disable pooling (see larger, more detailed maps)\n","# net.pool = nn.Identity(); net.force_5x5 = True\n","\n","# 4) Compare pooling types\n","# net.pool = nn.AvgPool2d(2,2)  # vs MaxPool2d(2,2)\n"]},{"cell_type":"markdown","id":"a694c9cd","metadata":{"id":"a694c9cd"},"source":["**TODO:**\n","\n","* **Nombre de filtres** : plus de filtres = plus de cartes/variété.\n","* **Taille du noyau** : plus grand = contexte spatial plus large.\n","* **Pooling ON/OFF** :  OFF = cartes plus grandes, plus de détails.\n","* **Type de pooling** : Max garde les pics, Avg lisse.\n","\n"," Si une erreur “size mismatch” arrive sur `fc1`, c’est normal (la taille change) : revenez aux valeurs d’origine **ou** insérez temporairement un `nn.AdaptiveAvgPool2d(1)` avant le `flatten`.\n","\n"," ---\n","**TODO:**\n","\n","* **Number of filters:** more filters = more feature maps/variety.\n","* **Kernel size:** larger = wider spatial context.\n","* **Pooling ON/OFF:** OFF = larger feature maps, more details.\n","* **Pooling type:** Max keeps peaks, Avg smooths.\n","\n","If a “size mismatch” error happens on `fc1`, that’s normal (the spatial size changed): revert to the original values or temporarily insert an `nn.AdaptiveAvgPool2d(1)` before the `flatten`."]},{"cell_type":"markdown","id":"39af0929","metadata":{"id":"39af0929"},"source":["### 3. Définir une fonction de perte / 3. *Define a loss function*"]},{"cell_type":"markdown","id":"61a8379e","metadata":{"id":"61a8379e"},"source":["Utilisons une perte d'entropie croisée de classification et un SGD avec impulsion. / *Let’s use a Classification Cross-Entropy loss and SGD with momentum.*"]},{"cell_type":"code","execution_count":null,"id":"35324983","metadata":{"id":"35324983"},"outputs":[],"source":["import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"]},{"cell_type":"markdown","id":"0352b54d","metadata":{"id":"0352b54d"},"source":["### 4. Entraîner le réseau sur les données d'entraînement / 4. *Train the network on the training data*"]},{"cell_type":"markdown","id":"3d6cdc11","metadata":{"id":"3d6cdc11"},"source":["C'est là que les choses deviennent intéressantes. Il suffit de boucler sur notre itérateur de données, d'alimenter le réseau avec les entrées et de l'optimiser. / *This is when things start to get interesting. We simply have to loop over our data iterator, and feed the inputs to the network and optimize.*"]},{"cell_type":"code","execution_count":null,"id":"7dfdbae6","metadata":{"id":"7dfdbae6"},"outputs":[],"source":["for epoch in range(2):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","\n","        running_loss += loss.item()\n","        if i % 2000 == 1999:    # print every 2000 mini-batches\n","            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n","            running_loss = 0.0\n","\n","print('Finished Training')"]},{"cell_type":"markdown","id":"bb463cf1","metadata":{"id":"bb463cf1"},"source":["TODO (in optimizer):\n","* **Learning rate** : `1e-2 / 1e-3 / 5e-4`\n","* **Epochs** : 1–3 pour la démo\n","* **Batch size** : petit = rapide/bruité ; grand = plus stable\n","\n","---"]},{"cell_type":"markdown","id":"7ef734e9","metadata":{"id":"7ef734e9"},"source":["Option : On sauvegarde le modèle / *we save the model*"]},{"cell_type":"code","execution_count":null,"id":"5b4be826","metadata":{"id":"5b4be826"},"outputs":[],"source":["PATH = './cifar_net.pth'\n","torch.save(net.state_dict(), PATH)"]},{"cell_type":"markdown","id":"e8f5ffe3","metadata":{"id":"e8f5ffe3"},"source":["### 5. Tester le réseau sur les données de test / 5. *Test the network on the test data*"]},{"cell_type":"markdown","id":"798e1868","metadata":{"id":"798e1868"},"source":["Nous avons entraîné le réseau à deux reprises sur l'ensemble de données d'entraînement. Nous devons cependant vérifier si le réseau a réellement appris quelque chose.\n","\n","Nous vérifierons cela en prédisant l'étiquette de classe générée par le réseau neuronal et en la comparant à la réalité. Si la prédiction est correcte, nous ajouterons l'échantillon à la liste des prédictions correctes.\n","\n","OK, première étape. Illustrons une image de l'ensemble de test pour nous familiariser.\n","\n","-----\n","*We have trained the network for 2 passes over the training dataset. But we need to check if the network has learnt anything at all.*\n","\n","*We will check this by predicting the class label that the neural network outputs, and checking it against the ground-truth. If the prediction is correct, we add the sample to the list of correct predictions.*\n","\n","*Okay, first step. Let us display an image from the test set to get familiar.*"]},{"cell_type":"code","execution_count":null,"id":"da74cd47","metadata":{"id":"da74cd47"},"outputs":[],"source":["dataiter = iter(testloader)\n","images, labels = next(dataiter)\n","\n","# print images\n","imshow(torchvision.utils.make_grid(images))\n","print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"]},{"cell_type":"markdown","id":"daa48236","metadata":{"id":"daa48236"},"source":["Option : On charge le modèle sauvegardé / *We load the saved model*"]},{"cell_type":"code","execution_count":null,"id":"24323d5e","metadata":{"id":"24323d5e"},"outputs":[],"source":["net = Net()\n","net.load_state_dict(torch.load(PATH, weights_only=True))"]},{"cell_type":"code","execution_count":null,"id":"8e7c4c87","metadata":{"id":"8e7c4c87"},"outputs":[],"source":["outputs = net(images)"]},{"cell_type":"markdown","id":"f3ef88e0","metadata":{"id":"f3ef88e0"},"source":["Les sorties correspondent aux énergies des 10 classes. Plus l'énergie d'une classe est élevée, plus le réseau considère que l'image appartient à cette classe. Prenons donc l'indice de l'énergie la plus élevée :\n","\n","*The outputs are energies for the 10 classes. The higher the energy for a class, the more the network thinks that the image is of the particular class. So, let’s get the index of the highest energy:*"]},{"cell_type":"code","execution_count":null,"id":"590c4e6d","metadata":{"id":"590c4e6d"},"outputs":[],"source":["_, predicted = torch.max(outputs, 1)\n","\n","print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n","                              for j in range(4)))"]},{"cell_type":"markdown","id":"b70c1920","metadata":{"id":"b70c1920"},"source":["Les résultats semblent plutôt bons. Voyons comment le réseau fonctionne sur l’ensemble des données.\n","\n","*The results seem pretty good. Let us look at how the network performs on the whole dataset.*"]},{"cell_type":"code","execution_count":null,"id":"84b31dab","metadata":{"id":"84b31dab"},"outputs":[],"source":["correct = 0\n","total = 0\n","# since we're not training, we don't need to calculate the gradients for our outputs\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        # calculate outputs by running images through the network\n","        outputs = net(images)\n","        # the class with the highest energy is what we choose as prediction\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"]},{"cell_type":"markdown","id":"339d4aea","metadata":{"id":"339d4aea"},"source":["[CIFAR-10] Cela semble bien meilleur que le hasard, qui est précis à 10 % (en choisissant une classe au hasard parmi 10 classes). On dirait que le réseau a appris quelque chose. Hmmm, quelles sont les classes qui ont bien fonctionné et celles qui n'ont pas bien fonctionné :\n","\n","*That looks way better than chance, which is 10% accuracy (randomly picking a class out of 10 classes). Seems like the network learnt something. Hmmm, what are the classes that performed well, and the classes that did not perform well:*"]},{"cell_type":"code","execution_count":null,"id":"3812995f","metadata":{"id":"3812995f"},"outputs":[],"source":["# prepare to count predictions for each class\n","correct_pred = {classname: 0 for classname in classes}\n","total_pred = {classname: 0 for classname in classes}\n","\n","# again no gradients needed\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predictions = torch.max(outputs, 1)\n","        # collect the correct predictions for each class\n","        for label, prediction in zip(labels, predictions):\n","            if label == prediction:\n","                correct_pred[classes[label]] += 1\n","            total_pred[classes[label]] += 1\n","\n","\n","# print accuracy for each class\n","for classname, correct_count in correct_pred.items():\n","    accuracy = 100 * float(correct_count) / total_pred[classname]\n","    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"]},{"cell_type":"markdown","id":"41ce6a6f","metadata":{"id":"41ce6a6f"},"source":["### 6. Explorer le modèle avec grad-cam / 6. *Explore the model with grad-cam*"]},{"cell_type":"markdown","id":"7b3b6d1f","metadata":{"id":"7b3b6d1f"},"source":["But : afficher ce que “regardent” les couches conv1 et conv2 d’un CNN en visualisant leurs feature maps.\n","\n","Comment :\n","1. Choix du device (GPU si dispo).\n","2. Pose de forward hooks sur conv1 et conv2 pour capturer les activations lors d’un passage avant.\n","3. Passage d’un batch en eval() → les activations sont stockées.\n","4. Affichage des n premiers canaux de conv1 (grilles de cartes [H×W]) pour la première image du batch.\n","5. Nettoyage : retrait des hooks.\n","\n","**TODO (dans la définition du modèle): changer le nombre de filtres, la taille de kernel, (dés)activer le pooling, et observer comment les cartes évoluent (plus ou moins de détails, textures différentes).**\n","\n","----\n","Goal: display what the conv1 and conv2 layers of a CNN “see” by visualizing their feature maps.\n","\n","How:\n","1. Choose the device (GPU if available).\n","2. Place forward hooks on conv1 and conv2 to capture activations during a forward pass.\n","3. Pass a batch through eval() → activations are stored.\n","4. Display the first n channels of conv1 (map grids [H×W]) for the first image in the batch.\n","5. Clean up: remove the hooks.\n","\n","**TODO (during the model definition): change the number of filters, the kernel size, (de)activate pooling, and observe how the maps evolve (more or less detail, different textures).**"]},{"cell_type":"code","execution_count":null,"id":"4aedd5a3","metadata":{"id":"4aedd5a3"},"outputs":[],"source":["import torch, matplotlib.pyplot as plt\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using:\", device)\n","\n","model = net  # your already-defined/trained model\n","\n","# ======= CONFIG PARAMS (modify these for the demo) =======\n","LAYER_NAME      = \"conv1\"     # TODO: \"conv1\" or \"conv2\"\n","IMG_INDEX       = 0           # TODO: which image in the batch to display (0, 1, 2, ...)\n","MAX_MAPS        = 8          # TODO: how many feature maps to show (e.g., 8, 12, 16, 32)\n","CMAP            = \"gray\"      # TODO: colormap (\"gray\", \"magma\", \"viridis\", ...)\n","NORMALIZE_MAPS  = True        # TODO: per-map normalization for better contrast\n","HOOK_AFTER_POOL = False       # TODO: set True to hook AFTER pooling (if model.pool exists)\n","\n","# Activation collector (avoid naming this 'targets' to prevent conflicts with Grad-CAM)\n","acts = {}\n","def save_activation(name):\n","    def hook(module, inp, out):\n","        acts[name] = out.detach().cpu()\n","    return hook\n","\n","# Choose which layer to hook\n","if HOOK_AFTER_POOL and hasattr(model, \"pool\"):\n","    layer = model.pool\n","    name  = \"after_pool\"\n","else:\n","    layer = getattr(model, LAYER_NAME)  # expects model.conv1 / model.conv2\n","    name  = LAYER_NAME\n","\n","# Register the hook\n","h = layer.register_forward_hook(save_activation(name))\n","\n","# One forward pass to populate 'acts'\n","model.eval()\n","x, y = next(iter(testloader))          # assumes 'testloader' exists\n","_ = model(x.to(device))\n","\n","# Get feature maps for the chosen image\n","fm = acts[name][IMG_INDEX]             # [C, H, W]\n","n  = min(MAX_MAPS, fm.shape[0])\n","\n","# Plot\n","fig, axs = plt.subplots(1, n, figsize=(n*2, 2))\n","for i in range(n):\n","    m = fm[i]\n","    if NORMALIZE_MAPS:\n","        m = (m - m.min()) / (m.max() - m.min() + 1e-8)\n","    axs[i].imshow(m.numpy(), cmap=CMAP)\n","    axs[i].axis(\"off\")\n","fig.suptitle(f\"{name} — showing {n}/{fm.shape[0]} maps (image #{IMG_INDEX})\")\n","plt.tight_layout()\n","plt.show()\n","\n","# Clean up\n","h.remove()\n"]},{"cell_type":"markdown","id":"9f0b3538","metadata":{"id":"9f0b3538"},"source":["Ce que tu vois est une grille de cartes d’activation produites par conv1 pour la première image du batch. Chaque vignette correspond à la réponse d’un filtre : zones claires = forte activation, sombres = faible. Le hook est posé avant ReLU, donc les motifs peuvent paraître ternes/bruités ; après quelques epochs, on voit souvent des détecteurs de bords/textures. Le nombre de vignettes = out_channels de conv1, et leur taille dépend du kernel/padding (et du pooling s’il intervient ensuite).\n","\n","*You’re looking at a grid of activation maps from conv1 for the first image in the batch.\n","Each tile is one filter’s response: bright = strong activation, dark = weak. The hook is pre-ReLU, so patterns may look faint/noisy; after a few epochs you typically see edge/texture detectors. The count of tiles equals conv1’s out_channels, and their spatial size depends on the kernel/padding (and pooling downstream).*\n"]},{"cell_type":"markdown","id":"bdf85adc","metadata":{"id":"bdf85adc"},"source":["### Plus d'exploration du grad-cam / *More Grad-cam exploration*"]},{"cell_type":"code","execution_count":null,"id":"f09f72bc","metadata":{"id":"f09f72bc"},"outputs":[],"source":["# === Drop-in utilities: Grad-CAM + feature maps ===\n","\n","import torch, torch.nn as nn\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from typing import Optional, List, Dict\n","\n","# 1) Safe import for pytorch-grad-cam\n","try:\n","    from pytorch_grad_cam import GradCAM\n","    from pytorch_grad_cam.utils.image import show_cam_on_image\n","    from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n","except Exception:\n","    import sys, subprocess\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"grad-cam\"])\n","    from pytorch_grad_cam import GradCAM\n","    from pytorch_grad_cam.utils.image import show_cam_on_image\n","    from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n","\n","# 2) Helpers\n","def get_device(model: nn.Module) -> torch.device:\n","    return next(model.parameters()).device\n","\n","def find_last_conv(model: nn.Module) -> nn.Module:\n","    last = None\n","    for m in model.modules():\n","        if isinstance(m, nn.Conv2d):\n","            last = m\n","    if last is None:\n","        raise RuntimeError(\"No nn.Conv2d layer found in the model.\")\n","    return last\n","\n","def tensor_to_rgb(img: torch.Tensor, mean: Optional[List[float]]=None, std: Optional[List[float]]=None) -> np.ndarray:\n","    \"\"\"\n","    img: [1 or C, H, W] or [C,H,W] tensor on any device. Returns float RGB [H,W,3] in [0,1].\n","    \"\"\"\n","    if img.dim() == 4:  # [1,C,H,W] -> [C,H,W]\n","        img = img[0]\n","    img = img.detach().float().cpu()\n","    if mean is not None and std is not None:\n","        mean_t = torch.tensor(mean).view(-1,1,1)\n","        std_t  = torch.tensor(std).view(-1,1,1)\n","        img = img * std_t + mean_t\n","    # clamp and ensure 3 channels\n","    if img.shape[0] == 1:\n","        img = img.repeat(3,1,1)\n","    elif img.shape[0] != 3:\n","        # take first 3 if more (best-effort for exotic inputs)\n","        img = img[:3]\n","    img = img.clamp(0,1)\n","    rgb = img.permute(1,2,0).numpy()\n","    return rgb\n","\n","# 3) Feature maps via forward hooks\n","class FeatureMapCatcher:\n","    def __init__(self, model: nn.Module, layers: Dict[str, nn.Module]):\n","        self.model = model\n","        self.layers = layers\n","        self.activations: Dict[str, torch.Tensor] = {}\n","        self._hooks = []\n","\n","    def _mk_hook(self, name):\n","        def hook(module, inp, out):\n","            self.activations[name] = out.detach().cpu()\n","        return hook\n","\n","    def register(self):\n","        self.remove()  # just in case\n","        for name, layer in self.layers.items():\n","            self._hooks.append(layer.register_forward_hook(self._mk_hook(name)))\n","\n","    def remove(self):\n","        for h in self._hooks:\n","            h.remove()\n","        self._hooks = []\n","\n","# 4) Run Grad-CAM\n","@torch.no_grad()\n","def predict_classes(model: nn.Module, x: torch.Tensor) -> List[int]:\n","    model.eval()\n","    logits = model(x)\n","    return logits.argmax(1).tolist()\n","\n","def run_gradcam(\n","    model: nn.Module,\n","    batch_x: torch.Tensor,\n","    target_layer: Optional[nn.Module] = None,\n","    mean: Optional[List[float]] = None,\n","    std: Optional[List[float]] = None,\n","    targets: Optional[List[ClassifierOutputTarget]] = None,\n","    index: int = 0,\n","    show: bool = True,\n","):\n","    \"\"\"\n","    Compute CAM for one image in a batch, overlay on RGB, and optionally display.\n","    - If targets=None, CAM uses top-1 class per image.\n","    - If target_layer=None, uses the last Conv2d found in the model.\n","    \"\"\"\n","    device = get_device(model)\n","    x = batch_x.to(device)\n","\n","    target_layer = target_layer or find_last_conv(model)\n","\n","    with GradCAM(model=model, target_layers=[target_layer]) as cam:\n","        grayscale_cams = cam(input_tensor=x, targets=targets)  # [N,H,W]\n","\n","    # Prepare background RGB for overlay\n","    rgb = tensor_to_rgb(batch_x[index:index+1], mean=mean, std=std)\n","    vis = show_cam_on_image(rgb, grayscale_cams[index], use_rgb=True)\n","\n","    if show:\n","        plt.figure(figsize=(4,4))\n","        plt.imshow(vis); plt.axis(\"off\"); plt.title(\"Grad-CAM overlay\")\n","        plt.show()\n","\n","    return grayscale_cams[index], vis\n"]},{"cell_type":"code","execution_count":null,"id":"660c0821","metadata":{"id":"660c0821"},"outputs":[],"source":["# 0) Mean/Std if you normalize (otherwise set mean/std=None)\n","mean = [0.4914, 0.4822, 0.4465]\n","std  = [0.2023, 0.1994, 0.2010]\n","\n","# 1) Grab a batch\n","images, labels = next(iter(testloader))\n","\n","# 2) (Optional) Visualize feature maps from two layers\n","\n","#    =TODO: -> adapt the layer names to your CNN (e.g., model.features[0], model.layer1[0].conv1, etc.)\n","layers_to_peek = {\n","    \"convA\": find_last_conv(model)  # you can put another conv here if you prefer\n","}\n","# Example: layers_to_peek = {\"conv1\": model.conv1, \"conv2\": model.conv2}\n","\n","catcher = FeatureMapCatcher(model, layers_to_peek)\n","catcher.register()\n","_ = model(images.to(get_device(model)))  # forward to populate catcher.activations\n","catcher.remove()\n","\n","# Display the first n maps for the first image\n","for name, act in catcher.activations.items():\n","    fm = act[0]                    # [C,H,W] for image 0\n","    n = min(16, fm.shape[0])\n","    fig, axs = plt.subplots(1, n, figsize=(n*2, 2))\n","    for i in range(n):\n","        axs[i].imshow(fm[i].numpy(), cmap=\"gray\")\n","        axs[i].axis(\"off\")\n","    fig.suptitle(f\"Feature maps: {name}\", y=1.05)\n","    plt.show()\n","\n","# 3) Grad-CAM (top-1 class automatically)\n","_ = run_gradcam(\n","    model=model,\n","    batch_x=images,          # <— your batch\n","    target_layer=None,       # auto: last conv of the model\n","    mean=mean, std=std,      # None/None if you did not normalize\n","    targets=None,            # top-1 per image\n","    index=0,                 # shows image 0 of the batch\n","    show=True\n",")\n","\n","# 4) (Option) Grad-CAM on a specified class, e.g., the predicted class for each image\n","preds = predict_classes(model, images.to(get_device(model)))\n","cam_targets = [ClassifierOutputTarget(c) for c in preds]\n","_ = run_gradcam(model, images, targets=cam_targets, mean=mean, std=std, index=0, show=True)\n"]},{"cell_type":"code","execution_count":null,"id":"733910ea","metadata":{"id":"733910ea"},"outputs":[],"source":["from pytorch_grad_cam import GradCAM\n","from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n","import torch, torch.nn as nn\n","\n","def find_last_conv(model: nn.Module) -> nn.Module:\n","    last = None\n","    for m in model.modules():\n","        if isinstance(m, nn.Conv2d):\n","            last = m\n","    if last is None:\n","        raise RuntimeError(\"No nn.Conv2d layer found in the model.\")\n","    return last\n","\n","\n","images, labels = next(iter(testloader))\n","\n","device = next(model.parameters()).device\n","batch_x = images.to(device)\n","\n","target_layer = find_last_conv(model)\n","\n","with GradCAM(model=model, target_layers=[target_layer]) as cam:\n","    grayscale_cams = cam(input_tensor=batch_x, targets=None)  # np.ndarray [N,H,W]\n","\n","print(type(grayscale_cams), grayscale_cams.shape)  # vérif rapide\n"]},{"cell_type":"markdown","source":["More grad-cam"],"metadata":{"id":"w04HhnyQwqwS"},"id":"w04HhnyQwqwS"},{"cell_type":"code","execution_count":null,"id":"ac6f8160","metadata":{"id":"ac6f8160"},"outputs":[],"source":["import torch, numpy as np\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from pytorch_grad_cam.utils.image import show_cam_on_image\n","\n","# --- Helpers (if not already defined) ---\n","def to_rgb(img, mean=None, std=None):\n","    \"\"\"img: [1,C,H,W] or [C,H,W] -> np.float32 [H,W,3] in [0,1]\"\"\"\n","    if img.dim() == 4:\n","        img = img[0]\n","    img = img.detach().float().cpu()\n","    if (mean is not None) and (std is not None):\n","        m = torch.tensor(mean).view(-1,1,1)\n","        s = torch.tensor(std).view(-1,1,1)\n","        img = img * s + m\n","    img = img.clamp(0, 1)\n","    if img.shape[0] == 1:  # grayscale -> RGB\n","        img = img.repeat(3,1,1)\n","    elif img.shape[0] > 3:\n","        img = img[:3]\n","    return img.permute(1,2,0).numpy()\n","\n","def show_cam_triptych(rgb, cam, title_left=\"Original\", title_mid=\"Heatmap\", title_right=\"Overlay\", suptitle=None):\n","    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n","    overlay = show_cam_on_image(rgb, cam, use_rgb=True)\n","    fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n","    axs[0].imshow(rgb);     axs[0].set_title(title_left);  axs[0].axis(\"off\")\n","    axs[1].imshow(cam, cmap=\"jet\"); axs[1].set_title(title_mid);   axs[1].axis(\"off\")\n","    axs[2].imshow(overlay); axs[2].set_title(title_right); axs[2].axis(\"off\")\n","    if suptitle: fig.suptitle(suptitle)\n","    plt.tight_layout(); plt.show()\n","\n","# --- Display params (adapt to YOUR normalization pipeline) ---\n","# If you normalize with 0.5 (like in your STL-10 code):\n","MEAN = [0.5, 0.5, 0.5]; STD = [0.5, 0.5, 0.5]\n","# If you use canonical CIFAR-10 stats, replace with:\n","# MEAN = [0.4914, 0.4822, 0.4465]; STD = [0.2023, 0.1994, 0.2010]\n","\n","# --- Pick an image from the batch and prepare data ---\n","i = 2  # which image to display\n","rgb = to_rgb(images[i:i+1], mean=MEAN, std=STD)\n","\n","cam = grayscale_cams[i]\n","if isinstance(cam, torch.Tensor):\n","    cam = cam.detach().cpu().numpy()\n","cam = cam.astype(np.float32)\n","\n","# If the CAM size differs from the image size, resize it properly\n","H, W = rgb.shape[:2]\n","if cam.shape != (H, W):\n","    cam_t = torch.from_numpy(cam)[None, None]               # [1,1,h,w]\n","    cam = F.interpolate(cam_t, size=(H, W), mode=\"bilinear\", align_corners=False).squeeze().numpy()\n","\n","# Class name (labels may live on the GPU)\n","cls_idx = int(labels[i].detach().cpu().item()) if torch.is_tensor(labels) else int(labels[i])\n","label_name = classes[cls_idx] if isinstance(classes, (list, tuple)) else str(cls_idx)\n","\n","# --- Side-by-side display: original / heatmap / overlay ---\n","show_cam_triptych(rgb, cam, suptitle=f\"Class: {label_name}\")\n"]},{"cell_type":"code","execution_count":null,"id":"0d634e2b","metadata":{"id":"0d634e2b"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"TP","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}